{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b361c3c-9abc-43e8-9b4a-b4e28b8d313f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchvision import transforms\n",
    "from torchvision.models import densenet121\n",
    "\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2de6049-1ef6-4231-be17-cc9ef9ba9ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"[Env] device={device}, cuda_available={torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"[Env] GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "PHASEA_BEST = \"your best weights\"\n",
    "CKPT_3Y = \"your previous deep learning model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20da589e-02e0-4c14-a61d-a3bfe2e5110d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "IMSIZE = 456\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std  = [0.229, 0.224, 0.225]\n",
    "\n",
    "t_train = transforms.Compose([\n",
    "    transforms.Resize((IMSIZE, IMSIZE)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=5),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.05),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "t_eval = transforms.Compose([\n",
    "    transforms.Resize((IMSIZE, IMSIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "#Dataset\n",
    "class SurvDataset(Dataset):\n",
    "    def __init__(self, df_part, X_clin, img_dir, transform):\n",
    "        self.df = df_part.reset_index(drop=True)\n",
    "        self.Xc = X_clin.astype(\"float32\", copy=False)\n",
    "        self.img_dir = img_dir\n",
    "        self.tfm = transform\n",
    "        assert len(self.df) == len(self.Xc), \n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        row = self.df.iloc[i]\n",
    "        fname = str(row[\"image_name\"])\n",
    "        path = os.path.join(self.img_dir, fname)\n",
    "        if not os.path.isfile(path):\n",
    "            for ext in [\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\"]:\n",
    "                p2 = os.path.join(self.img_dir, fname + ext)\n",
    "                if os.path.isfile(p2):\n",
    "                    path = p2; break\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        xi = self.tfm(img)\n",
    "        xc = torch.from_numpy(self.Xc[i])\n",
    "        d  = torch.tensor(max(float(row[\"duration_months\"]), 1e-3), dtype=torch.float32)\n",
    "        e  = torch.tensor(int(row[\"event\"]), dtype=torch.float32)\n",
    "        return xi, xc, d, e, path\n",
    "\n",
    "#DataLoaders\n",
    "BATCH_TRAIN = 32\n",
    "BATCH_EVAL  = 32\n",
    "NUM_WORKERS = 4\n",
    "PIN_MEMORY  = True\n",
    "\n",
    "ds_train = SurvDataset(train_df, X_train_clin, IMG_DIRS[\"train\"], t_train)\n",
    "ds_val   = SurvDataset(val_df,   X_val_clin,   IMG_DIRS[\"valid\"], t_eval)\n",
    "ds_test  = SurvDataset(test_df,  X_test_clin,  IMG_DIRS[\"test\"],  t_eval)\n",
    "\n",
    "dl_train = DataLoader(ds_train, batch_size=BATCH_TRAIN, shuffle=True,\n",
    "                      num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "dl_val   = DataLoader(ds_val,   batch_size=BATCH_EVAL,  shuffle=False,\n",
    "                      num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "dl_test  = DataLoader(ds_test,  batch_size=BATCH_EVAL,  shuffle=False,\n",
    "                      num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "\n",
    "print(f\"[DL] train={len(ds_train)} | val={len(ds_val)} | test={len(ds_test)} | clin_dim={clin_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf60807-8a0f-4910-bf6c-0989e6dbd198",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import densenet121\n",
    "\n",
    "class ImgBranch(nn.Module):\n",
    "    def __init__(self, backbone, drop=0.4):\n",
    "        super().__init__()\n",
    "        self.features = backbone.features                 \n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.proj = nn.Sequential(                         \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(drop),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = F.relu(x, inplace=True)\n",
    "        x = self.pool(x)\n",
    "        return self.proj(x)\n",
    "\n",
    "class ClinBranch(nn.Module):\n",
    "    def __init__(self, in_dim, drop=0.3):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, 128), nn.BatchNorm1d(128), nn.ReLU(inplace=True), nn.Dropout(drop),\n",
    "            nn.Linear(128, 64),  nn.BatchNorm1d(64),  nn.ReLU(inplace=True), nn.Dropout(drop),\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "class MultiModalCox(nn.Module):\n",
    "    def __init__(self, img_backbone, clin_in, comb=128, drop=0.3):\n",
    "        super().__init__()\n",
    "        self.img  = ImgBranch(img_backbone)\n",
    "        self.clin = ClinBranch(clin_in)\n",
    "        self.fuse = nn.Sequential(\n",
    "            nn.Linear(512+64, comb), nn.BatchNorm1d(comb), nn.ReLU(inplace=True), nn.Dropout(drop)\n",
    "        )\n",
    "        self.head = nn.Linear(comb, 1, bias=False)  \n",
    "    def forward(self, xi, xc):\n",
    "        zi = self.img(xi); zc = self.clin(xc)\n",
    "        z  = self.fuse(torch.cat([zi, zc], dim=1))\n",
    "        return self.head(z).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bc0deb-079c-4e3f-a634-bb6866d53e4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_model_from_phaseA():\n",
    "    backbone = densenet121(weights=None)\n",
    "    model = MultiModalCox(backbone, clin_in=clin_dim).to(device)\n",
    "    sd = torch.load(PHASEA_BEST, map_location='cpu')\n",
    "    missing, unexpected = model.load_state_dict(sd, strict=False)\n",
    "    if missing:   print(\"[INFO] Missing keys:\", missing)\n",
    "    if unexpected:print(\"[INFO] Unexpected keys:\", unexpected)\n",
    "    return model\n",
    "\n",
    "def set_bn_eval(m):\n",
    "    if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d)):\n",
    "\n",
    "def apply_unfreeze(model: nn.Module, mode: str):\n",
    "    for p in model.img.features.parameters():\n",
    "        p.requires_grad = False\n",
    "    model.img.features.apply(set_bn_eval)\n",
    "\n",
    "    names = [\"denseblock4\", \"transition3\"]               \n",
    "    if mode == \"block43\":                                \n",
    "        names += [\"denseblock3\", \"transition2\"]\n",
    "\n",
    "    for name, module in model.img.features.named_children():\n",
    "        if name in names:\n",
    "            for p in module.parameters():\n",
    "                p.requires_grad = True\n",
    "            module.apply(set_bn_eval)\n",
    "\n",
    "from pycox.models.loss import CoxPHLoss\n",
    "cox_loss = CoxPHLoss()\n",
    "\n",
    "model_smoke = build_model_from_phaseA()\n",
    "apply_unfreeze(model_smoke, mode=\"block4\")  \n",
    "\n",
    "xi, xc, d, e, _ = next(iter(dl_train))\n",
    "xi, xc, d, e = xi.to(device), xc.to(device), d.to(device), e.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    log_risk = model_smoke(xi, xc)\n",
    "    loss = cox_loss(log_risk, d, e)\n",
    "\n",
    "print(f\"[Smoke Model] log_risk=({log_risk.min().item():.3f},{log_risk.max().item():.3f})  loss={loss.item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6353f4-86cb-47fb-a767-671662b8d147",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_surv(model, dl_train, dl_eval, times=None):\n",
    "    import numpy as np, pandas as pd\n",
    "    from pycox.evaluation import EvalSurv\n",
    "\n",
    "    CLIP = 20.0  \n",
    "\n",
    "    def _predict(loader):\n",
    "        model.eval()\n",
    "        R, D, E = [], [], []\n",
    "        for xi, xc, d, e, _ in loader:\n",
    "            xi, xc = xi.to(device), xc.to(device)\n",
    "            lr = model(xi, xc).detach().cpu().numpy().astype(np.float64)\n",
    "            lr = np.clip(lr, -CLIP, CLIP)                            \n",
    "            dn = d.detach().cpu().numpy().astype(np.float64)\n",
    "            en = e.detach().cpu().numpy().astype(np.int64)\n",
    "            R.append(lr); D.append(dn); E.append(en)\n",
    "        r = np.concatenate(R); d = np.concatenate(D); e = np.concatenate(E)\n",
    "\n",
    "        m = np.isfinite(r) & np.isfinite(d) & np.isfinite(e)\n",
    "        r, d, e = r[m], d[m], e[m]\n",
    "        d = np.maximum(d, 1e-3)\n",
    "        return r, d, e\n",
    "\n",
    "    r_tr, d_tr, e_tr = _predict(dl_train)\n",
    "    n_evt_tr = int((e_tr == 1).sum())\n",
    "    if n_evt_tr == 0:\n",
    "        print(\"[Eval] no events in training set after filtering → return NaN\")\n",
    "        return float('nan'), float('nan')\n",
    "\n",
    "    order = np.argsort(d_tr, kind='mergesort')\n",
    "    t_tr, e_tr, r_tr = d_tr[order], e_tr[order], r_tr[order]\n",
    "    uniq_evt = np.unique(t_tr[e_tr == 1])\n",
    "\n",
    "    H0_vals, T_vals = [], []\n",
    "    for ut in uniq_evt:\n",
    "        at_risk = np.exp(r_tr[t_tr >= ut]).sum()\n",
    "        d_i = ((t_tr == ut) & (e_tr == 1)).sum()\n",
    "        H0_vals.append(d_i / max(at_risk, 1e-12))\n",
    "        T_vals.append(ut)\n",
    "    H0_vals = np.cumsum(np.asarray(H0_vals, dtype=np.float64))\n",
    "    T_vals  = np.asarray(T_vals, dtype=np.float64)\n",
    "\n",
    "    r_ev, d_ev, e_ev = _predict(dl_eval)\n",
    "    n_evt_ev = int((e_ev == 1).sum())\n",
    "\n",
    "    if times is None:\n",
    "        lo_candidates = [1.0]\n",
    "        if T_vals.size: lo_candidates.append(T_vals.min())\n",
    "        if d_ev.size:   lo_candidates.append(d_ev.min())\n",
    "        lo = max(lo_candidates)\n",
    "\n",
    "        hi_candidates = []\n",
    "        if T_vals.size: hi_candidates.append(T_vals.max())\n",
    "        if d_ev.size:   hi_candidates.append(d_ev.max())\n",
    "        hi = min(hi_candidates) if hi_candidates else lo + 1.0\n",
    "\n",
    "        if not np.isfinite(lo) or not np.isfinite(hi) or hi <= lo:\n",
    "            lo = float(T_vals.min()) if T_vals.size else 1.0\n",
    "            hi = float(T_vals.max()) if T_vals.size else lo + 1.0\n",
    "            if hi <= lo: hi = lo + 1.0\n",
    "\n",
    "        times = np.linspace(lo, hi, 80).astype(np.float64)\n",
    "\n",
    "    Ht = np.interp(times, T_vals, H0_vals, left=0.0, right=(H0_vals[-1] if H0_vals.size else 0.0))\n",
    "    surv_rows = [np.exp(-Ht * np.exp(ri)) for ri in r_ev]\n",
    "    surv = pd.DataFrame(np.vstack(surv_rows).T, index=times) \n",
    "\n",
    "    es = EvalSurv(surv, d_ev, e_ev, censor_surv='km')\n",
    "\n",
    "    try:\n",
    "        uno = float(es.concordance_td('uno'))\n",
    "    except Exception:\n",
    "        try:\n",
    "            uno = float(es.concordance_td())  # antolini\n",
    "        except Exception:\n",
    "            uno = float('nan')\n",
    "\n",
    "    try:\n",
    "        ibs = float(es.integrated_brier_score(times))\n",
    "    except Exception:\n",
    "        try:\n",
    "            lo2, hi2 = np.percentile(d_ev, [5, 95])\n",
    "            if not np.isfinite(lo2) or not np.isfinite(hi2) or hi2 <= lo2:\n",
    "                raise ValueError\n",
    "            times2 = np.linspace(lo2, hi2, 60).astype(np.float64)\n",
    "            Ht2 = np.interp(times2, T_vals, H0_vals, left=0.0, right=(H0_vals[-1] if H0_vals.size else 0.0))\n",
    "            surv2 = pd.DataFrame(np.vstack([np.exp(-Ht2 * np.exp(ri)) for ri in r_ev]).T, index=times2)\n",
    "            es2 = EvalSurv(surv2, d_ev, e_ev, censor_surv='km')\n",
    "            ibs = float(es2.integrated_brier_score(times2))\n",
    "        except Exception:\n",
    "            ibs = float('nan')\n",
    "\n",
    "    print(f\"[Eval] train_events={n_evt_tr}, eval_events={n_evt_ev}, \"\n",
    "          f\"time_range=[{times.min():.1f},{times.max():.1f}], uno={uno if np.isfinite(uno) else np.nan:.4f}, \"\n",
    "          f\"ibs={ibs if np.isfinite(ibs) else np.nan:.4f}\")\n",
    "    return uno, ibs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f78606-6465-4214-a2d2-94a14cb7c2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import optuna\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from pycox.models.loss import CoxPHLoss\n",
    "import os, copy\n",
    "\n",
    "cox_loss = CoxPHLoss()\n",
    "\n",
    "def _nan_guard_batch(xi, xc, d, e):\n",
    "\n",
    "    for t in (xi, xc, d, e):\n",
    "        bad = ~torch.isfinite(t)\n",
    "        if bad.any(): t[bad] = 0\n",
    "    e = (e > 0.5).float()\n",
    "    d = torch.clamp(d, min=1e-3)\n",
    "    return xi, xc, d, e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33116b7-36a5-4b20-916a-6c10f6685e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_trial(model, dl_train, dl_val,\n",
    "                    lr_back, lr_head, weight_decay,\n",
    "                    max_epochs=12, patience=5, trial=None):\n",
    "\n",
    "    params_back, params_head = [], []\n",
    "    for n, p in model.named_parameters():\n",
    "        if not p.requires_grad:\n",
    "            continue\n",
    "        if \"img.features\" in n:\n",
    "            params_back.append(p)\n",
    "        else:\n",
    "            params_head.append(p)\n",
    "\n",
    "    opt = AdamW([\n",
    "        {\"params\": params_back, \"lr\": lr_back, \"weight_decay\": weight_decay},\n",
    "        {\"params\": params_head, \"lr\": lr_head, \"weight_decay\": weight_decay},\n",
    "    ])\n",
    "\n",
    "    scheduler = ReduceLROnPlateau(opt, mode='max', factor=0.5, patience=2, verbose=False)\n",
    "\n",
    "    best = {\"uno\": -1.0, \"ibs\": 1e9, \"state\": None, \"epoch\": -1}\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for ep in range(1, max_epochs+1):\n",
    "        model.train()\n",
    "        run_loss, num_ok = 0.0, 0\n",
    "\n",
    "        for xi, xc, d, e, _ in dl_train:\n",
    "            xi, xc, d, e = _nan_guard_batch(xi, xc, d, e)\n",
    "            xi, xc, d, e = xi.to(device), xc.to(device), d.to(device), e.to(device)\n",
    "\n",
    "            if e.sum().item() < 1:\n",
    "                continue\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            log_risk = model(xi, xc)\n",
    "            log_risk = torch.clamp(log_risk, min=-20.0, max=20.0)  \n",
    "            if not torch.isfinite(log_risk).all():\n",
    "                continue\n",
    "            loss = cox_loss(log_risk, d, e)\n",
    "            if not torch.isfinite(loss):\n",
    "                continue\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step()\n",
    "\n",
    "            run_loss += float(loss.detach().cpu().item())\n",
    "            num_ok += 1\n",
    "\n",
    "        if num_ok == 0:\n",
    "            if trial: trial.set_user_attr(\"note\", \"no valid batches (too many no-event batches)\")\n",
    "            return best\n",
    "\n",
    "        try:\n",
    "            uno, ibs = evaluate_surv(model, dl_train, dl_val)\n",
    "        except Exception as ex:\n",
    "            uno, ibs = float('nan'), float('nan')\n",
    "\n",
    "        scheduler.step(uno if np.isfinite(uno) else -1.0)\n",
    "\n",
    "        improved = False\n",
    "        if np.isfinite(uno) and (uno > best[\"uno\"] or (abs(uno-best[\"uno\"])<1e-5 and np.isfinite(ibs) and ibs < best[\"ibs\"])):\n",
    "            best.update({\"uno\": float(uno), \"ibs\": float(ibs), \"state\": copy.deepcopy(model.state_dict()), \"epoch\": ep})\n",
    "            improved = True\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if trial:\n",
    "            trial.report(float(uno) if np.isfinite(uno) else -1.0, ep)\n",
    "            if trial.should_prune():\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            break\n",
    "\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bc5d46-adfe-447c-9d30-560c0209b5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch, json\n",
    "\n",
    "best_params = {\n",
    "    \"unfreeze\": \"your best params\",\n",
    "    \"lr_backbone\": #your best params,\n",
    "    \"lr_head\": #your best params,\n",
    "    \"weight_decay\": #your best params,\n",
    "}\n",
    "print(\"[Fixed Phase-B] best_params:\", best_params)\n",
    "\n",
    "model = build_model_from_phaseA()\n",
    "apply_unfreeze(model, best_params[\"unfreeze\"])\n",
    "\n",
    "best_fixed = train_one_trial(\n",
    "    model,\n",
    "    dl_train, dl_val,\n",
    "    lr_back=best_params[\"lr_backbone\"],\n",
    "    lr_head=best_params[\"lr_head\"],\n",
    "    weight_decay=best_params[\"weight_decay\"],\n",
    "    max_epochs=15,\n",
    "    patience=5,\n",
    "    trial=None,  \n",
    ")\n",
    "\n",
    "final_pth = os.path.join(SAVE_DIR, \"name of best model\")\n",
    "if best_fixed[\"state\"] is not None:\n",
    "    torch.save(best_fixed[\"state\"], final_pth)\n",
    "    print(f\"[Fixed Phase-B] saved best to: {final_pth}\")\n",
    "    print(f\"[Fixed Phase-B] Val UnoC={best_fixed['uno']:.4f}, IBS={best_fixed['ibs']:.4f}, epoch={best_fixed['epoch']}\")\n",
    "else:\n",
    "    print(\"warning: no best \")\n",
    "\n",
    "\n",
    "model_test = build_model_from_phaseA()\n",
    "sd = torch.load(final_pth, map_location=\"cpu\")\n",
    "_ = model_test.load_state_dict(sd, strict=False)\n",
    "model_test = model_test.to(device)\n",
    "\n",
    "uno_te, ibs_te = evaluate_surv(model_test, dl_train, dl_test)\n",
    "print(f\"[Test] UnoC={uno_te:.4f}, IBS={ibs_te:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
