{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5042e7e0-87f2-4a12-a5f3-be6e21874285",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, copy, random, numpy as np, pandas as pd\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.models import densenet121\n",
    "from PIL import Image\n",
    "\n",
    "from pycox.evaluation import EvalSurv\n",
    "from pycox.models.loss import CoxPHLoss\n",
    "from torchtuples import optim as ttoptim\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "def seed_everything(s=2025):\n",
    "    random.seed(s); np.random.seed(s); torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
    "seed_everything(2025)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0d4575-db65-4087-a1cf-9a3fb821d1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_clin = np.load(NPY_TRAIN).astype(\"float32\")\n",
    "X_val_clin   = np.load(NPY_VAL).astype(\"float32\")\n",
    "X_test_clin  = np.load(NPY_TEST).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7028649-e557-41a4-89f8-89270f192778",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMSIZE = 456\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std  = [0.229, 0.224, 0.225]\n",
    "\n",
    "t_train = transforms.Compose([\n",
    "    transforms.Resize((IMSIZE, IMSIZE)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "t_eval = transforms.Compose([\n",
    "    transforms.Resize((IMSIZE, IMSIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda84851-635f-46c7-bd71-9262b2272c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurvDataset(Dataset):\n",
    "    def __init__(self, df_part: pd.DataFrame, X_clin: np.ndarray, img_dir: str, transform):\n",
    "        \n",
    "        self.df = df_part\n",
    "        self.Xc = X_clin\n",
    "        self.img_dir = img_dir\n",
    "        self.tfm = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        row = self.df.iloc[i]\n",
    "        fname = row[\"PIC_Name\"]\n",
    "        img_path = os.path.join(self.img_dir, str(fname))\n",
    "        if not os.path.isfile(img_path):\n",
    "            tried = [img_path]\n",
    "            exts = [\".jpg\", \".png\", \".jpeg\", \".bmp\", \".tif\", \".tiff\"]\n",
    "            hit = None\n",
    "            for ext in exts:\n",
    "                p2 = os.path.join(self.img_dir, str(fname) + ext)\n",
    "                tried.append(p2)\n",
    "                if os.path.isfile(p2):\n",
    "                    hit = p2; break\n",
    "            if hit is None:\n",
    "                raise FileNotFoundError(f\"could not find image\")\n",
    "            img_path = hit\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        xi = self.tfm(img)\n",
    "\n",
    "        xc = torch.from_numpy(self.Xc[i])\n",
    "\n",
    "        dval = float(row[\"duration_time\"])\n",
    "        if not np.isfinite(dval):\n",
    "            dval = 1e-3\n",
    "        dur = torch.tensor(max(dval, 1e-3), dtype=torch.float32)\n",
    "        evt = torch.tensor(int(row[\"end\"]), dtype=torch.float32)\n",
    "        return xi, xc, dur, evt, img_path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59db6e0c-5c29-4d72-80fb-f2be1654d46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_TRAIN = 24\n",
    "BATCH_EVAL  = 32\n",
    "NUM_WORKERS = 4\n",
    "PIN_MEMORY  = True\n",
    "\n",
    "ds_train = SurvDataset(train_df, X_train_clin, IMG_DIRS[\"train\"], t_train)\n",
    "ds_val   = SurvDataset(val_df,   X_val_clin,   IMG_DIRS[\"valid\"], t_eval)\n",
    "ds_test  = SurvDataset(test_df,  X_test_clin,  IMG_DIRS[\"test\"],  t_eval)\n",
    "\n",
    "dl_train = DataLoader(ds_train, batch_size=BATCH_TRAIN, shuffle=True,  num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "dl_val   = DataLoader(ds_val,   batch_size=BATCH_EVAL,  shuffle=False, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "dl_test  = DataLoader(ds_test,  batch_size=BATCH_EVAL,  shuffle=False, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf497f7c-204c-4a1e-9dd0-2c0ac4c456ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "CKPT_3Y = \"your own model\"\n",
    "\n",
    "class ImgBranch(nn.Module):\n",
    "    def __init__(self, backbone, drop=0.4):\n",
    "        super().__init__()\n",
    "        self.features = backbone.features\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(drop),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = nn.functional.relu(x, inplace=True)\n",
    "        x = self.pool(x)\n",
    "        return self.proj(x)\n",
    "\n",
    "class ClinBranch(nn.Module):\n",
    "    def __init__(self, in_dim, drop=0.3):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, 128), nn.BatchNorm1d(128), nn.ReLU(inplace=True), nn.Dropout(drop),\n",
    "            nn.Linear(128, 64),  nn.BatchNorm1d(64),  nn.ReLU(inplace=True), nn.Dropout(drop),\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "class MultiModalCox(nn.Module):\n",
    "    def __init__(self, img_backbone, clin_in, comb=128, drop=0.3):\n",
    "        super().__init__()\n",
    "        self.img = ImgBranch(img_backbone)\n",
    "        self.clin = ClinBranch(clin_in)\n",
    "        self.fuse = nn.Sequential(\n",
    "            nn.Linear(512+64, comb), nn.BatchNorm1d(comb), nn.ReLU(inplace=True), nn.Dropout(drop)\n",
    "        )\n",
    "        self.head = nn.Linear(comb, 1, bias=False)  # log-risk\n",
    "    def forward(self, xi, xc):\n",
    "        zi = self.img(xi); zc = self.clin(xc)\n",
    "        z = self.fuse(torch.cat([zi, zc], dim=1))\n",
    "        return self.head(z).squeeze(1)\n",
    "\n",
    "def load_backbone_from_cls(ckpt_path):\n",
    "    bb = densenet121(weights=None)\n",
    "    sd = torch.load(ckpt_path, map_location='cpu')\n",
    "    if isinstance(sd, dict) and 'state_dict' in sd:\n",
    "        sd = sd['state_dict']\n",
    "    sd = {k.replace('module.', ''): v for k,v in sd.items()\n",
    "          if 'classifier' not in k}  \n",
    "    bb.load_state_dict(sd, strict=False)\n",
    "    return bb\n",
    "\n",
    "backbone = load_backbone_from_cls(CKPT_3Y)\n",
    "model = MultiModalCox(backbone, clin_in=14).to(device) \n",
    "for p in model.img.features.parameters():\n",
    "    p.requires_grad = False\n",
    "model.img.features.train(False)\n",
    "\n",
    "cox_loss = CoxPHLoss()\n",
    "opt = torch.optim.AdamW([p for p in model.parameters() if p.requires_grad], lr=3e-4, weight_decay=1e-6)\n",
    "\n",
    "xi, xc, d, e, _ = next(iter(dl_train))\n",
    "xi, xc, d, e = xi.to(device), xc.to(device), d.to(device), e.to(device)\n",
    "opt.zero_grad(set_to_none=True)\n",
    "log_risk = model(xi, xc)\n",
    "loss = cox_loss(log_risk, d, e)\n",
    "loss.backward()\n",
    "nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "opt.step()\n",
    "print(f\"Smoke test OK. one-batch loss={loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462bab68-640c-4419-8bc0-2d43428dbbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def _predict(model, loader):\n",
    "    model.eval()\n",
    "    R, D, E = [], [], []\n",
    "    for xi, xc, d, e, _ in loader:\n",
    "        xi, xc = xi.to(device), xc.to(device)\n",
    "        lr = model(xi, xc).detach().cpu().numpy()\n",
    "        R.append(lr); D.append(d.numpy()); E.append(e.numpy())\n",
    "    return np.concatenate(R), np.concatenate(D), np.concatenate(E)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_surv(model, dl_train, dl_eval, times=None):\n",
    "    import numpy as np, pandas as pd\n",
    "    from pycox.evaluation import EvalSurv\n",
    "\n",
    "    def _predict(loader):\n",
    "        model.eval()\n",
    "        R, D, E = [], [], []\n",
    "        for xi, xc, d, e, _ in loader:\n",
    "            xi, xc = xi.to(device), xc.to(device)\n",
    "            lr = model(xi, xc).detach().cpu().numpy().astype(np.float64)   \n",
    "            dn = d.detach().cpu().numpy().astype(np.float64)               \n",
    "            en = e.detach().cpu().numpy().astype(np.int64)                 \n",
    "            R.append(lr); D.append(dn); E.append(en)\n",
    "        r = np.concatenate(R); d = np.concatenate(D); e = np.concatenate(E)\n",
    "        m = np.isfinite(r) & np.isfinite(d) & np.isfinite(e)\n",
    "        r, d, e = r[m], d[m], e[m]\n",
    "        d = np.maximum(d, 1e-3) \n",
    "        return r, d, e\n",
    "\n",
    "    r_tr, d_tr, e_tr = _predict(dl_train)\n",
    "    n_evt_tr = int((e_tr == 1).sum())\n",
    "    if n_evt_tr == 0:\n",
    "        return float('nan'), float('nan')\n",
    "\n",
    "    order = np.argsort(d_tr, kind='mergesort')\n",
    "    t_tr, e_tr, r_tr = d_tr[order], e_tr[order], r_tr[order]\n",
    "    uniq_evt = np.unique(t_tr[e_tr == 1])\n",
    "\n",
    "    H0_vals, T_vals = [], []\n",
    "    for ut in uniq_evt:\n",
    "        at_risk = np.exp(r_tr[t_tr >= ut]).sum()\n",
    "        d_i = ((t_tr == ut) & (e_tr == 1)).sum()\n",
    "        H0_vals.append(d_i / max(at_risk, 1e-12))\n",
    "        T_vals.append(ut)\n",
    "    H0_vals = np.cumsum(np.asarray(H0_vals, dtype=np.float64))\n",
    "    T_vals  = np.asarray(T_vals, dtype=np.float64)\n",
    "\n",
    "    r_ev, d_ev, e_ev = _predict(dl_eval)\n",
    "    n_evt_ev = int((e_ev == 1).sum())\n",
    "\n",
    "    if times is None:\n",
    "        lo_candidates = [1.0]\n",
    "        if T_vals.size: lo_candidates.append(T_vals.min())\n",
    "        if d_ev.size:   lo_candidates.append(d_ev.min())\n",
    "        lo = max(lo_candidates)\n",
    "\n",
    "        hi_candidates = []\n",
    "        if T_vals.size: hi_candidates.append(T_vals.max())\n",
    "        if d_ev.size:   hi_candidates.append(d_ev.max())\n",
    "        hi = min(hi_candidates) if hi_candidates else lo + 1.0\n",
    "\n",
    "        if not np.isfinite(lo) or not np.isfinite(hi) or hi <= lo:\n",
    "            lo = float(T_vals.min()) if T_vals.size else 1.0\n",
    "            hi = float(T_vals.max()) if T_vals.size else lo + 1.0\n",
    "            if hi <= lo: hi = lo + 1.0\n",
    "\n",
    "        times = np.linspace(lo, hi, 80).astype(np.float64)\n",
    "\n",
    "    Ht_grid = np.interp(times, T_vals, H0_vals, left=0.0, right=(H0_vals[-1] if H0_vals.size else 0.0))\n",
    "    surv_rows = [np.exp(-Ht_grid * np.exp(ri)) for ri in r_ev]\n",
    "    surv = pd.DataFrame(np.vstack(surv_rows).T, index=times)\n",
    "\n",
    "    es = EvalSurv(surv, d_ev, e_ev, censor_surv='km')\n",
    "\n",
    "    try:\n",
    "        uno_val = es.concordance_td('uno')\n",
    "        uno = float(uno_val)\n",
    "    except Exception as ex:\n",
    "        try:\n",
    "            uno_alt = es.concordance_td() \n",
    "            uno = float(uno_alt)\n",
    "        except Exception:\n",
    "            uno = float('nan')\n",
    "\n",
    "    try:\n",
    "        ibs_val = es.integrated_brier_score(times)\n",
    "        ibs = float(ibs_val)\n",
    "    except Exception:\n",
    "        try:\n",
    "            lo2, hi2 = np.percentile(d_ev, [5, 95])\n",
    "            if not np.isfinite(lo2) or not np.isfinite(hi2) or hi2 <= lo2:\n",
    "                raise ValueError\n",
    "            times2 = np.linspace(lo2, hi2, 60).astype(np.float64)\n",
    "            Ht2 = np.interp(times2, T_vals, H0_vals, left=0.0, right=(H0_vals[-1] if H0_vals.size else 0.0))\n",
    "            surv_rows2 = [np.exp(-Ht2 * np.exp(ri)) for ri in r_ev]\n",
    "            surv2 = pd.DataFrame(np.vstack(surv_rows2).T, index=times2)\n",
    "            es2 = EvalSurv(surv2, d_ev, e_ev, censor_surv='km')\n",
    "            ibs = float(es2.integrated_brier_score(times2))\n",
    "        except Exception:\n",
    "            ibs = float('nan')\n",
    "\n",
    "    print(f\"[Eval] train_events={n_evt_tr}, eval_events={n_evt_ev}, \"\n",
    "          f\"time_range=[{times.min():.1f},{times.max():.1f}], uno={uno:.4f}, ibs={ibs if np.isfinite(ibs) else np.nan}\")\n",
    "\n",
    "    return uno, ibs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ddf64d-7055-42a7-b324-0db3ad54c4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _nan_guard_batch(xi, xc, d, e, name=\"train\"):\n",
    "\n",
    "    import torch\n",
    "    def clamp_finite(t, tname):\n",
    "        t = t.clone()\n",
    "\n",
    "        mask = ~torch.isfinite(t)\n",
    "        if mask.any():\n",
    "            t[mask] = 0\n",
    "            print(f\"[WARN] {name}:{tname} had {mask.sum().item()} non-finite; replaced with 0\")\n",
    "        return t\n",
    "    xi = clamp_finite(xi, \"xi\")\n",
    "    xc = clamp_finite(xc, \"xc\")\n",
    "    d  = clamp_finite(d,  \"d\")\n",
    "    e  = clamp_finite(e,  \"e\")\n",
    "\n",
    "    e = (e > 0.5).float()\n",
    "    d = torch.clamp(d, min=1e-3)\n",
    "\n",
    "    if (e.sum() == 0) or (e.sum() == e.numel()):\n",
    "        pass\n",
    "\n",
    "    return xi, xc, d, e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0312caac-38ff-4dc3-9944-efb0000d3529",
   "metadata": {},
   "outputs": [],
   "source": [
    "xi, xc, d, e, _ = next(iter(dl_train))\n",
    "print(\"d stats:\", d.min().item(), d.max().item(), torch.isnan(d).any().item())\n",
    "print(\"e stats:\", e.unique(return_counts=True))\n",
    "print(\"xc stats:\", torch.isnan(xc).any().item(), torch.isinf(xc).any().item(),\n",
    "      float(xc.min()), float(xc.max()))\n",
    "with torch.no_grad():\n",
    "    out = model(xi.to(device), xc.to(device)).detach().cpu()\n",
    "print(\"log_risk stats:\", float(out.min()), float(out.max()),\n",
    "      torch.isnan(out).any().item(), torch.isinf(out).any().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1358e33b-23ce-4e11-9114-70f30d452169",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_phase_A(epochs=6):\n",
    "    best = {'uno': -1, 'ibs': 1e9, 'state': None}\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        run_loss = 0.0\n",
    "        num_ok = 0\n",
    "\n",
    "        for xi, xc, d, e, _ in dl_train:\n",
    "\n",
    "            xi, xc, d, e = _nan_guard_batch(xi, xc, d, e, name=f\"ep{ep}\")\n",
    "            xi, xc, d, e = xi.to(device), xc.to(device), d.to(device), e.to(device)\n",
    "\n",
    "            if e.sum().item() < 1:\n",
    "                continue\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            log_risk = model(xi, xc)\n",
    "\n",
    "            if not torch.isfinite(log_risk).all():\n",
    "                continue\n",
    "\n",
    "            loss = cox_loss(log_risk, d, e)\n",
    "            \n",
    "            if not torch.isfinite(loss):\n",
    "                continue\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step()\n",
    "\n",
    "            run_loss += float(loss.detach().cpu().item()) \n",
    "            num_ok += 1\n",
    "\n",
    "        if num_ok == 0:\n",
    "            print(f\"[WARN] Epoch {ep}: no valid batches (likely too many no-event batches). \"\n",
    "                  f\"Try larger batch_size or event-stratified sampler.\")\n",
    "            tr_loss_str = \"nan\"\n",
    "        else:\n",
    "            tr_loss_str = f\"{run_loss:.3f}\"\n",
    "\n",
    "        try:\n",
    "            uno, ibs = evaluate_surv(model, dl_train, dl_val)\n",
    "        except Exception as ex:\n",
    "            print(f\"[WARN] evaluate_surv failed at epoch {ep}: {ex}\")\n",
    "            uno, ibs = float('nan'), float('nan')\n",
    "\n",
    "        print(f\"Epoch {ep}: train_loss={tr_loss_str} | Val UnoC={uno} | IBS={ibs}\")\n",
    "\n",
    "        if np.isfinite(uno) and (uno > best['uno'] or (abs(uno-best['uno'])<1e-5 and np.isfinite(ibs) and ibs < best['ibs'])):\n",
    "            best.update({'uno': float(uno), 'ibs': float(ibs), 'state': copy.deepcopy(model.state_dict())})\n",
    "\n",
    "    if best['state'] is not None:\n",
    "        torch.save(best['state'], \"save best weights\")\n",
    "    else:\n",
    "        print(\"[WARN] No valid best state to save (all epochs invalid).\")\n",
    "\n",
    "    return best\n",
    "\n",
    "best = train_phase_A(epochs=6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
